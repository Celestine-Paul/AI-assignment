# 🕵️ AI in the Real World — Judge the Bot  

Hello reader! Today, I put on my detective hat 🎩 to investigate how AI behaves in the wild. My mission: find bias, lack of transparency, or unfairness — and suggest fixes. Let’s dive in.  

---

## 📝 Case 1: The Hiring Bot  

**What’s happening:**  
A company uses an AI tool to screen job applicants. It looks at resumes and decides who moves to the interview stage.  

**What’s problematic:**  
The bot often rejects women with career gaps (e.g., maternity leave). That’s unfair bias — penalizing someone for caregiving roles. It reduces diversity and keeps workplaces unequal.  

**One improvement idea:**  
Retrain the AI with **fairness-aware data** that includes career gaps as normal life events. Add human review checks so applicants aren’t rejected only by the bot.  

---

## 📝 Case 2: The School Proctoring AI  

**What’s happening:**  
A proctoring system monitors students’ eye movements during exams. If a student looks away from the screen “too much,” it flags them as cheating.  

**What’s problematic:**  
Neurodivergent students (e.g., ADHD, autism) may naturally move or avoid eye contact. The AI wrongly accuses them of dishonesty. That’s discrimination and harms student trust.  

**One improvement idea:**  
Shift from **eye-tracking only** to multiple signals (keyboard activity, unusual browser activity, etc.). Also, include a fairness review board to check how often neurodivergent students are flagged.  

---

## ✨ Wrap-up  

Being a Responsible AI Inspector means asking: *Is the AI fair? Transparent? Respectful of privacy?* These small fixes can make technology more human-centered ❤️.  

---

👩‍💻 Published as part of my PLP assignment.  
