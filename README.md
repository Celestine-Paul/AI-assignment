# ğŸ•µï¸ AI in the Real World â€” Judge the Bot  

Hello reader! Today, I put on my detective hat ğŸ© to investigate how AI behaves in the wild. My mission: find bias, lack of transparency, or unfairness â€” and suggest fixes. Letâ€™s dive in.  

---

## ğŸ“ Case 1: The Hiring Bot  

**Whatâ€™s happening:**  
A company uses an AI tool to screen job applicants. It looks at resumes and decides who moves to the interview stage.  

**Whatâ€™s problematic:**  
The bot often rejects women with career gaps (e.g., maternity leave). Thatâ€™s unfair bias â€” penalizing someone for caregiving roles. It reduces diversity and keeps workplaces unequal.  

**One improvement idea:**  
Retrain the AI with **fairness-aware data** that includes career gaps as normal life events. Add human review checks so applicants arenâ€™t rejected only by the bot.  

---

## ğŸ“ Case 2: The School Proctoring AI  

**Whatâ€™s happening:**  
A proctoring system monitors studentsâ€™ eye movements during exams. If a student looks away from the screen â€œtoo much,â€ it flags them as cheating.  

**Whatâ€™s problematic:**  
Neurodivergent students (e.g., ADHD, autism) may naturally move or avoid eye contact. The AI wrongly accuses them of dishonesty. Thatâ€™s discrimination and harms student trust.  

**One improvement idea:**  
Shift from **eye-tracking only** to multiple signals (keyboard activity, unusual browser activity, etc.). Also, include a fairness review board to check how often neurodivergent students are flagged.  

---

## âœ¨ Wrap-up  

Being a Responsible AI Inspector means asking: *Is the AI fair? Transparent? Respectful of privacy?* These small fixes can make technology more human-centered â¤ï¸.  

---

ğŸ‘©â€ğŸ’» Published as part of my PLP assignment.  
